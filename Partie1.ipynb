{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Dinarque/INALCO_Inalco_M2_langage_de_scripts_2024_eleves/blob/main/TP/TP3_les_sombres_secrets_de_l_INALCO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7b4ZBQgzo_a"
   },
   "source": [
    "# TP 3 : Les sombres secrets de l'INALCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yy4Ovp3RzuAz"
   },
   "source": [
    "La plupart des sites webs proposent une expérience de navigation guidée et souvent clairement structurée (section, sous section, article...) permettant à l'utilisateur d'accéder aux informations qu'on l'autorise à voir.\n",
    "Derrière les coulisses, un site web ressemble plutôt à une forme d'arboresence de fichiers ,( différents \"/\" dans les url) une arborescence d'url menant chacun à une page web unique, souvent connectée aux autres mais pas forcéments.\n",
    "Par exemple une url peut mener directement à un fichier (pdf d'un cours dont le lien est donné sur la page moodle, le lien peut envoyer vers un fichier stocké à l'extérieur ou dans un site). Des url peuvent stocker des pages périmées non accessibles depuis les autres liens du site et qui ne sont plus référencées (anciennes versions de brochures pour l'année scolaire 2021-2022...) ou encore à des pages auxquelles ne peuvent accéder que des utilisateurs identifiés (si leur accès est correctement protégé).\n",
    "Ainsi le site naviguable que l'utilisateur explore n'est souvent que le haut de l'iceberg. Toutes les autres pages web du site non trivialement accessibles peuvent pourtant contenir des informations très intéressantes notamment d'un point de vue stratégique et sont potentiellement une mine d'information dans le cadre de l'OSINT.\n",
    "Nous allons apprendre à collecter ces informations cachées et les rendre exploitables au moyen de techniques modernes de NLP.\n",
    "\n",
    "PS : les techniques vues dans ce TP ne doivent être employées que dans des buts éthiques. L'utilisateur de l'outil est responsable de ses actes je ne fais que vous apprendre à mettre ensemble des pièces de puzzle déjà en libre accès sur github ou dans les cordes de tout programmeur compétent.\n",
    "\n",
    "Comme vous le savez tous, les sites des universités ne sont pas forcément toujours bien organisés ou mis à jour. Nous allons donc construire un moteur de recherche documentaire sur les cours de l'INALCO à partir des données du site officiel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96lulAlN2NYO"
   },
   "source": [
    "# Objectifs:\n",
    "* découvrir et expérimenter plusieurs techniques de crawling et de dorking pour apprendre à extraire les données pertinentes d'un site web\n",
    "* Concevoir un code orienté objet\n",
    "* Apprendre à traîter de grandes quantités de fichiers de type différents\n",
    "* apprendre à optimiser la performance de ses programmes en passant d'une architecture séquentielle à une architecture parallèlisée\n",
    "* apprendre les concepts basiques de la gestion de bases de données et de la recherche sémantique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwLVoY2717tb"
   },
   "source": [
    "# Partie 1 : Dorking and crawling your way to the data /6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxC06BtLBEaC"
   },
   "source": [
    "L'objectif de cette partie est de réaliser un crawling le plus complet possible du site de l'INALCO pour connecter des données pour nourrir notre modèle de recherche documentaire. On veut collecter le contenu de pages web, mais est particulièrement intéressé par des fichiers plus riches en contenu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j86DUU_B0pYO"
   },
   "source": [
    "# 1) Petite cartographie du site de l'Inalco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7LKWqYTBCEA"
   },
   "source": [
    "Dans un premier temps, il faut explorer le site web pour découvrir toutes les url librement accessibles pour l'utilisateur (principe du crawl).\n",
    "\n",
    "Pour ce faire je vous propose d'explorer le site web \"en profondeur\" (parcours depth first) à partir de l'url de départ https://www.inalco.fr/ . Le principe est de repérer tous les liens url internes (sur le même site) sur la page principale, de les stocker et de répéter cette opération sur les liens découverts (degré de profondeur suivant...).  Pour éviter que cette opération ne soit sans fin, on fixe un seuil de profondeur n.\n",
    "\n",
    "* écrivez une fonction qui prend en entrée l'url d'une page et renvoie en sortie la liste des urls contenues dans cette page (scraping).\n",
    "\n",
    "* écrivez une fonction (récursive ?) qui prend en entrée l'url d'une page et explore toutes les urls du site à une prodondeur n (c'est à dire que n clics maximum sont nécessaires pour atteindre la page depuis la page principale)  et renvoie la liste de toutes les url explorées.\n",
    "Souvenez vous des bonnes pratiques du scraping : ajoutez les headers pour permettre au site de vous identifier, ignorez les pages interdites par le fichier robots.txt et ajoutez un temps d'attente (time.wait...) entre le scraping de deux pages pour ne pas surcharger le site.\n",
    "\n",
    "* Testez votre fonction avec N=2 uniquement. Combien d'url sont renvoyées ?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import urllib.robotparser\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vérifie si le scraping est autorisé pour une URL donnée en consultant le fichier robots.txt\n",
    "def is_allowed_to_scrape(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "    rp.set_url(robots_url)\n",
    "    try:\n",
    "        rp.read()\n",
    "        # Vérifie si l'agent utilisateur par défaut (*) est autorisé à scrapper l'URL\n",
    "        return rp.can_fetch(\"*\", url)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la vérification de robots.txt: {e}\")\n",
    "        return True  # Par défaut, retourne True en cas d'erreur\n",
    "\n",
    "# Récupère les URL trouvées sur une page donnée si le scraping est autorisé\n",
    "def scrapURLs(url: str, headers: dict):\n",
    "    # Vérifie si l'URL peut être scrappée\n",
    "    if not is_allowed_to_scrape(url):\n",
    "        print(f\"{url} n'est pas autorisé à scrapper.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Effectue une requête GET sur l'URL\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors du scraping de {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Analyse le contenu HTML avec BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = set()\n",
    "\n",
    "    # Trouve tous les liens <a> avec un attribut href\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        full_url = urljoin(url, link['href'])  # Génère des URL absolues\n",
    "        links.add(full_url)\n",
    "\n",
    "    return list(links)\n",
    "\n",
    "# Fonction de scraping récursif pour collecter des liens sur plusieurs niveaux de profondeur\n",
    "def recursiveScrapper(url, depth=2, visited_urls=None, headers=None):\n",
    "    if visited_urls is None:\n",
    "        visited_urls = set()  # Ensemble des URLs déjà visitées\n",
    "        \n",
    "    if headers is None:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (compatible; MyScraper/1.0; +http://example.com/bot)\"\n",
    "        }\n",
    "        \n",
    "    # Arrête la récursion si la profondeur maximale est atteinte ou si l'URL a déjà été visitée\n",
    "    if depth <= 0 or url in visited_urls:\n",
    "        return visited_urls\n",
    "\n",
    "    visited_urls.add(url)  # Marque l'URL comme visitée\n",
    "    links = scrapURLs(url, headers)  # Récupère les liens sur la page\n",
    "\n",
    "    # Parcourt les liens trouvés et effectue un scraping récursif\n",
    "    for link in links:\n",
    "        if link not in visited_urls:\n",
    "            recursiveScrapper(link, depth - 1, visited_urls, headers)\n",
    "\n",
    "    return visited_urls\n",
    "\n",
    "all_links = recursiveScrapper(\"https://www.inalco.fr/\", 2)\n",
    "print(f\"Il y au total {len(all_links)} liens si on fait une profondeur de 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yE7T7X0auYnZ"
   },
   "source": [
    "# 2) How many files do we have up here ?\n",
    "\n",
    "\n",
    "Le dorking consiste à utiliser tout le potentiel des recherches Google pour trouver les informations d'intêrét.\n",
    "En effet, certaines commandes souvent peu connues des utilisateurs permettent d'avoir des résultats très différents.\n",
    "Pour plus de détails lire par exemple :\n",
    "https://www.recordedfuture.com/threat-intelligence-101/threat-analysis-techniques/google-dorks\n",
    "\n",
    "Parmi ces commandes les plus utiles sont:\n",
    "*  ‘filetype:’ qui permet de recherche un type de fichier en particulier\n",
    "* 'Site:' permet de ne rechercher que à l'intérieur d'un site\n",
    "* 'Inurl: ' force la présence d'un mot clé dans l'url\n",
    "* 'Ext: ' finds files with a certain extension\n",
    "\n",
    "* Après avoir lu la documentation de ces commandes, écrire une requête google qui renvoie tous les fichiers avec l'extension \"extension\" (variable) qui se cachent dans le site https://www.inalco.fr/\n",
    "écrivez une fonction qui renvoie pour chaque extension de fichier la query google à tapper pour trouver les fichiers avec cette extension sur le site de l'Inalco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Se1amf_Ew76s"
   },
   "outputs": [],
   "source": [
    "file_extensions = {\n",
    "    \".pdf\": \"Portable Document Format\",\n",
    "    \".docx\": \"Microsoft Word document (newer format)\",\n",
    "    \".doc\": \"Microsoft Word document (older format)\",\n",
    "    \".csv\": \"Comma-separated values file for tabular data\",\n",
    "    \".tsv\": \"Tab-separated values file for tabular data\",\n",
    "    \".jpg\": \"JPEG image file\",\n",
    "    \".png\": \"Portable Network Graphics image file\",\n",
    "    \".tiff\": \"Tagged Image File Format image\",\n",
    "    \".bmp\": \"Bitmap image file\",\n",
    "    \".ppt\": \"Microsoft PowerPoint presentation (older format)\",\n",
    "    \".pptx\": \"Microsoft PowerPoint presentation (newer format)\",\n",
    "    \".pptm\": \"Microsoft PowerPoint macro-enabled presentation\",\n",
    "    \".odt\": \"OpenDocument Text document\",\n",
    "}\n",
    "\n",
    "# Génère une requête Google pour rechercher des fichiers spécifiques sur un site donné\n",
    "def googleQuery(extension: str, site: str = \"https://www.inalco.fr/\"):\n",
    "    clean_extension = extension.lstrip(\".\")  # Supprime le point initial de l'extension\n",
    "    return f\"filetype:{clean_extension} site:{site} inurl:{clean_extension}\"\n",
    "\n",
    "print(googleQuery(\"pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qyko_TUyy3d2"
   },
   "source": [
    "* Google interdit explicitement dans ses conditions d'utilisation d'effectuer des recherches sur son moteur de façon automatisée par requête HTML et se réserve le droit de bannir votre adresse IP si vous le faites.\n",
    "La plupart du temps, le dorking se réalise à la main. Génétez vos requêtes avec votre fonction et testez les en les copiant collant dans google. Combien de fichiers peut-on trouver avec chaque extension de fichier ? Que conseillez-vous de faire pour la suite du travail ?\n",
    "\n",
    "\n",
    "* On a toutefois besoin d'obtenir l'url des fichiers se trouvant sur le site et donc de s'interfacer avec le moteur de recherche google depuis notre script pytohn. Nous ne sommes pas les premiers à nous être posés la question. Par exemple cette librairie permet de s'interfacer avec Google https://medium.com/@sagarydv002/google-search-in-python-a-beginners-guide-742472fec9cc mais son usage n'est pas conforme aux conditions d'utilisation de Google.\n",
    "heureusement google propose une API pour requêter son moteur de recherche. https://developers.google.com/custom-search/v1/overview?hl=fr\n",
    "L'usage de cette API est payant au delà de 100 requêtes par jour, mais vous n'avez pas besoin de plus que ça pour apprendre à vous en servir.\n",
    "\n",
    "Pour utiliser l'API il faut que vous créiez un compte et configuriez un custom search engine puis que vous requêtiez l'api RESTFUL en fournissant les pramètre suivants.\n",
    "\n",
    " params = {\n",
    "            \"q\": query,\n",
    "            \"key\": api_key\n",
    "            \"cx\": # id de votre custom search engine\n",
    "            \"num\": # nombre de résultats attendus.\n",
    "            \"start\": start_index,\n",
    "        }\n",
    "\n",
    "Lisez la documentation de cette API et écrivez une fonction qui retourne l'url des 100 premiers fichiers pdfs du site de l'INALCO (ce n'est pas si simple que ça, lisez les informations sur les paramètres \"num\" et \"start\" !)\n",
    "\n",
    "* L'API limite normalement le retour à 100 pages maximum. Ce n'est pas assez pour s'amuser ! Suggérez une idée pour collecter plus de documents ? (je ne vous demande pas de l'implémenter juste de réfléchir au problème)\n",
    "\n",
    "\n",
    "PS :  des packages github dédiés à l'OSINT se sont penchés sur la même questions que nous, je suis tombé par exemple pour celui ci https://github.com/opsdisk/metagoofil?tab=readme-ov-file\n",
    "Là encore il ne respecte pas les conditions d'utilisation de Google.\n",
    "Certaines solutions comme SERPAPI respectent les conditions de Google mais sont payantes (car utilisent l'API payante de Google dans les coulisses, j'imagine !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Il y a au total 9360 documents de PDF mais aucun d'autre type de document.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W56YgC06HuNr"
   },
   "outputs": [],
   "source": [
    "# Utilise l'API Google Custom Search pour récupérer des liens basés sur une requête\n",
    "def GoogleAPIScrapper(title2link: dict = None):\n",
    "    cle = \"AIzaSyBSjpDXvXj8pOKf7oCI-bzUEdwcT7ok-6o\"  # Clé API Google\n",
    "    query = googleQuery(\"pdf\")  # Génère une requête pour trouver des fichiers PDF\n",
    "    BasicURL = \"https://www.googleapis.com/customsearch/v1\"\n",
    "\n",
    "    if title2link is None:\n",
    "        title2link = {}\n",
    "\n",
    "    # Fonction interne pour récupérer des données paginées via l'API\n",
    "    def getData(start: int = 1):\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"key\": cle,\n",
    "            \"cx\": \"13d4c7b2e9d5f4176\",  # Identifiant du moteur de recherche personnalisé\n",
    "            \"num\": 10,  # Nombre maximum de résultats par requête\n",
    "            \"start\": start,\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(BasicURL, params=params)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erreur lors de la requête : {e}\")\n",
    "            return None\n",
    "\n",
    "    start = 1\n",
    "    max_results = 100  # Limite maximale de résultats\n",
    "    while start <= max_results:\n",
    "        data = getData(start)\n",
    "        if not data or \"items\" not in data:\n",
    "            break  # Arrête si aucune donnée ou résultats restants\n",
    "\n",
    "        # Parcourt les éléments de résultats et les ajoute à title2link\n",
    "        for item in data[\"items\"]:\n",
    "            title, link = item.__getitem__(\"title\"), item.__getitem__(\"link\")\n",
    "            if title and link:\n",
    "                title2link.__setitem__(title, link)\n",
    "\n",
    "        start += 10  # Passe à la page suivante\n",
    "\n",
    "    return title2link\n",
    "\n",
    "title2link = GoogleAPIScrapper()\n",
    "print(f\"Nombre total de résultats : {len(title2link)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ozQCkWFy8pR"
   },
   "source": [
    "# 3) Siphonner tout cela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyxKYz7WHvjh"
   },
   "source": [
    "Maintenant que l'on sait quoi et où collecter les données, il faut les récupérer et les stocker !\n",
    "\n",
    "* écrivez une fonction prennant en entrée l'url d'un pdf et qui télécharge le fichier dans un dossier nommé \"corpus_pdf\"\n",
    "\n",
    "* écrivez une fonction prennant en entrée une url classique et téléchargeant la page html dans un fichier html dans un dossier nommé \"corpus_html\"\n",
    "\n",
    "* Intégrez intelligemment ces fonctions dans votre code pour économiser le nombre de requêtes lancées sur le site\n",
    "\n",
    "* écrivez un script qui effectue un crawling avec une profondeur de 10 du site de l'inalco en sauvegardant les pages html et qui télécharge 3000 pdfs.\n",
    "NE LE LANCEZ PAS LE BUT N EST PAS DE METTRE A GENOUX LE SITE DE L UNIVERSITE !!!\n",
    "\n",
    "Je vous fournirai les données pour continuer le travail !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadPDF_HTML(url, index=1):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Vérifie si l'URL pointe vers un fichier PDF\n",
    "        if response.headers['Content-Type'] == 'application/pdf':\n",
    "            output_dir = \"corpus_pdf\"\n",
    "            with open(f\"{output_dir}/{index}.pdf\", 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            print(f\"Fichier PDF téléchargé et sauvegardé sous : {output_dir}/{index}.pdf\")\n",
    "        else:\n",
    "            # Sauvegarde une page HTML si ce n'est pas un PDF\n",
    "            output_dir = \"corpus_html\"\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            htmlTitle = soup.head.title.get_text() if soup.head.title else \"Unnamed\"\n",
    "            # Nettoie le titre du fichier HTML\n",
    "            htmlTitle = re.sub(r'[\\/:*?\"<>| ]', \"_\", htmlTitle)\n",
    "            htmlTitle = re.sub(r'_{2,}', \"_\", htmlTitle)\n",
    "            with open(f'{output_dir}/{htmlTitle}.html', 'w', encoding='utf-8') as file:\n",
    "                file.write(response.text)\n",
    "            print(f\"Page HTML téléchargée et sauvegardée sous : {output_dir}/{htmlTitle}.html\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors du téléchargement : {e}\")\n",
    "\n",
    "downloadPDF_HTML(\"https://www.inalco.fr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction principale pour exécuter le crawler\n",
    "def crawler():\n",
    "    depth10links = recursiveScrapper(\"https://www.inalco.fr/\", 10)  # Récupère des liens jusqu'à une profondeur de 10\n",
    "    index = 1\n",
    "    for link in depth10links:\n",
    "        downloadPDF_HTML(link, index)  # Télécharge le contenu des liens trouvés\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OX0ry9Oe3RS-"
   },
   "source": [
    "# BONUS Partie 1: (spoiler du genre de choses que l'on fera au semestre prochain)\n",
    "\n",
    "* Modifiez votre code de la question 1) pour trouver un moyen de stocker les renvois d'une page à une autre (le fait qu'une page web contienne un lien vers une autre page)\n",
    "* Servez vous de ces données pour construire un graphe de connectivité du site et trouvez\n",
    "* Déterminez quelles sont les pages centrales du site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytiLt3t_zdv2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxXALhiC2CoC"
   },
   "source": [
    "# Partie 2 : la Data Prep /10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_zc_QTwJgiN"
   },
   "source": [
    "Nous avons collecté des milliers de fichiers bruts. Le but est de nettoyer, enrichir puis stocker ces informations avant de pouvoir construire notre moteur de recherche documentaire.\n",
    "\n",
    "Nous allons pour celà adopter une façon de penser \"orientée objet\" et partir de la notion de document. Un document a un titre, une extension... ainsi qu'un certain nombre de méta données\n",
    "Il faudra\n",
    "- extraire le texte des documents\n",
    "- stocker ce contenu dans un objet Document adapté\n",
    "(- enrichir le document avec des informations intéressantes (NER))\n",
    "- Découper le document en des sections plus réduites (chunk) pour améliorer la performance du moteur de recherche documentaire.\n",
    "- Indexer les documents dans une base de données adaptées\n",
    "- S'intéresser aux performances du pipeline d'indexation et trouver une manière de l'observer\n",
    "- Trouver des astuces pour accélérer ce traitement des données en masse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7z1oQGGLRje"
   },
   "source": [
    "# 4) Extraire l'information pertinente des fichiers\n",
    "\n",
    "On peut être amené à traiter quantités de fichiers de type différent et souhaiterait disposer d'un traitement unitaire qui serait applicable à tous les fichiers.\n",
    "\n",
    "Nous allons principalement traiter des fichiers html et pdf, mais dans la vraie vie, l'idéal serait de disposer d'un processus pour extraire les informations de tout type de fichier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pFOGFvgQSxX"
   },
   "source": [
    "a) L'object Document\n",
    "\n",
    "Nous allons stocker les documents dans une classe Document adaptée. Un document représente le contenu d'un fichier informatique.  Il contient également des métadonnées utiles au requếtage (date de collecte...)\n",
    "\n",
    "* à Votre avis, quels sont les champs dont il faut doter l'objet Document ?\n",
    "\n",
    "* Créez une classe document, ainsi que les fonctions __init__(), __str__() , ainsi que _add(field_name, field_content) une méthode qui ajoute un paramètre à l'objet.\n",
    "\n",
    "* Implémentez une méthode qui calcule l'ID d'un document.\n",
    "L'id commencera par inalco{nmdedocindexe}{dateindexation au format j/m/a}{10 premières lettres hors caractères spéciauxstockerstocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEJr0z1BQRfb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2nrB0C9QSBW"
   },
   "source": [
    "\n",
    "b) Extraire le texte\n",
    "\n",
    "Je propose d'utiliser la librairie Unstructured pour réaliser cette extraction.\n",
    "https://github.com/Unstructured-IO\n",
    "\n",
    "Vous pouvez pour aller plus loin consulter ce cours https://learn.deeplearning.ai/courses/preprocessing-unstructured-data-for-llm-applications/lesson/1/introduction\n",
    "de deeplearning.ai un site que je recommande beaucoup pour notre discipline  !\n",
    "\n",
    "* écrivez une fonction qui prend en entrée un nom de fichier, qu'il soit .html ou .pdf et renvoie le texte\n",
    "\n",
    "NB : pour obtenir un texte unique pour chaque document, utilisez le séparateur [SEP].\n",
    "On pourra réfléchir à des choses plus raffinées par la suite.\n",
    "\n",
    "* écrivez une fonction qui prenne en entrée un path vers un fichier et renvoie l'objet Document stockant également son texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8unFcardLQzk",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dY31WWUXeMuT"
   },
   "source": [
    "# 5) Chunking des documents\n",
    "\n",
    "Les Documents sont bien trop longs pour que l'on puisse les utiliser d'un seul bloc pour effectuer une recherche sémantique. Il convient donc de les découper en plus petites unités significatives avant de les indexer\n",
    "\n",
    "* Renseignez vous sur les différentes méthodes de chunking existantes. Laquelle vous semble la plus pertintente ?\n",
    "\n",
    "* Puisque unstructured découpe déjà par rapport à la strucutre du document, nous allons exploiter le découpage déjà effectuer (traces des séparateurs [SEP] que je vous ai fait garder !) pour écrire une variante de chunking entre le structuré et le récursif.\n",
    "écrivez une fonction qui prend en entrée un texte et renvoie la liste de chunks.  Tant que le chunk ne dépasse pas n caractères (initialisé à 500) continue d'ajouter des segments du texte.  Si ajouter le segment de texte suivant fait dépasser cette limitation, on stocke le chunk et passe au suivant. Un segment de document dont la taille dépasse n est splitté à la phrase si besoin pour respecter la consigne de taille et scindé en plusieurs chunks.\n",
    "\n",
    "* Créez une classe chunk avec les champs qui vous semblent pertinents,  et la méthode __init__() et __str__().\n",
    "Tout chunk doit avoir un id qui est la concaténation de l'id du doc et de l'expression \"chunk{numéro du chunk à partir de 1\"\n",
    "\n",
    "* écrivez une fonction qui prend en entrée un Document et renvoie la liste des objets Chunks correspondant à ce document.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xwe-6V4Ss32n"
   },
   "source": [
    "# 6) Indexation de la base de donnée\n",
    "\n",
    "\n",
    "a) Configurer la base de données\n",
    "\n",
    "C'est merveilleux ! Nous savons à présent transformer des fichiers en chunks !\n",
    "Il ne nous reste plus qu'à les indexer dans une base de données.\n",
    "\n",
    "Pour ce faire nous allons choisir une base de donnée vectorielle, comme par exemple ChromaDB qui est open source et largement utilisée.\n",
    "\n",
    "https://docs.trychroma.com/getting-started\n",
    "\n",
    "https://www.datacamp.com/tutorial/chromadb-tutorial-step-by-step-guide (bon guide mais certains des codes donnés sont périmés)\n",
    "\n",
    "\n",
    "* Lisez les tutoriels et créez une base de donnée pour notre projet, nommée inalco.\n",
    "Ecrivez une fonction pour créer la base de donnée et éventuellement la purger si nécessaire.\n",
    "\n",
    "* écrivez une fonction qui permet de stocker un chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGg_8C4yyJ9q"
   },
   "source": [
    "b) THE script\n",
    "\n",
    "* Ecrivez une fonction qui prend en entrée un nom de fichier et la collection, crée le document et indexe les chunks dans la base\n",
    "\n",
    "* Ecrivez un script global qui prend en entrée un nom de DOSSIER, et traite les documents du dossier de façon séquentielle à l'aide de la fonction précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KffJcqUr2HCv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3T2PZVEfy1zS"
   },
   "source": [
    "# 7) Monitorer son programme\n",
    "\n",
    "Si vous lancez THE script précédent sur un dossier contenant des centaines de fichiers, vous vous rendrez vite compte que les choses ne vont pas se passer aussi vite que vous l'espériez.\n",
    "\n",
    "Il est temp d'évaluer les dégâts !\n",
    "\n",
    "* Faites un bon usage du logging et de la librairie tqdm pour rassurer l'utilisateur et lui dire que de bonnes choses sont en train de se passer.\n",
    "\n",
    "Il faut trouver un moyen de mesurer plusieurs paramètres : le temps total du traitement de tous les fichiers,le temps mis pour traiter chaque fichier, ainsi que repérer les problèmes potentiels (fichiers non traités ou ne contenant pas de texte...) pour informer l'utilisateur. Nous allons y aller progressivement\n",
    "\n",
    "* Créez un décorateur qui une fois appliqué à une fonction permet de mesurer son temps d'éxécution. Utilisez le sur le processus global\n",
    "\n",
    "* Adaptez le décorateur pour que l'information sur le temps de traitement d'un fichier (input de la fonction) lui soit associé et stocké.\n",
    "\n",
    "* Adaptez ce décorateur pour qu'il repère les erreurs de traitement (nombre de chunk indexé nul ou texte vide... sur un fichier et stocke l'information.\n",
    "\n",
    "* Faites en sorte que l'emploi de votre décorateur de monitoring produise un rapport indiquant le temps total de traitement, le temps moyen et médian de traitement par fichier ainsi que la liste des fichiers pour lesquels une erreur s'est produite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDy1HVa7gY18"
   },
   "source": [
    "# 8) Accélérer son programme à l'aide du multithreading\n",
    "\n",
    "* Utilisez des techniques de programmation asyncrone pour paralléliser le traitement des fichiers et accélérer votre programme.\n",
    "\n",
    "* Faites en sorte que la barre de progression tqdm affiche encore correctement la progression du traitement des fichiers (ce n'est pas trivial !)\n",
    "\n",
    "* Faites en sorte que le décorateur de monitoring foncitonne toujours correctement\n",
    "\n",
    "* En combien de temps tourne le nouveau programme  ? Quel est le pourcentage de temps gagné ? Le traitement individuel de chaque fichier a-t-il été ralenti ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GJVWBU9LZGN"
   },
   "source": [
    "# Bonus Partie 2 : IBM ?\n",
    "\n",
    "IBM vient de sortit une librairie de traitement des données\n",
    "https://github.com/IBM/unitxt\n",
    "Utilisez les composants de cette librairie pour remplacer ce que l'on a fait dans la partie 2 et comparez les résultats en terme de performance et de qualité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7MwycRF5yuK"
   },
   "source": [
    "# Partie 3 : construire le moteur d'indexation documentaire et son interface /4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tddE7gmWhnOc"
   },
   "source": [
    "# 9) Méthodes de requêtage de la base de données\n",
    "\n",
    "L'objectif final de tout ce travail est de construire une application permettant à un utilisateur de poser une question ou entrer des mots clés et de retrouver les documents et url les plus pertinents par rapport à sa recherche.\n",
    "On veut donc retrouver un document et pas seulement un chunk !\n",
    "\n",
    "* Qu'est ce que cela implique sur notre gestion de la base de données ?\n",
    "\n",
    "* Créez une fonction qui permette de retrouver k chunks à partir de la question de l'utilisateur.\n",
    "\n",
    "* Créez une fonction qui permette de récupérer le document complet à partir d'un chunk. NB : cela implique peut être des modifications du code en amont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRvAwr8giC82"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8zZttwmiDSr"
   },
   "source": [
    "# 10) Front de l'application\n",
    "\n",
    "Utilisez streamlit (ou autre framework de votre choix) pour réaliser un Front basique à cette application.\n",
    "\n",
    "* L'utilisateur doit pouvoir poser sa question dans un champs spécifique\n",
    "* Il doit pouvoir choisir le nombre de documents retournés (entre 1 et 20 ?)\n",
    "* Le document doit être bien paginé, l'url mise en avant\n",
    "* le segment correspondant au chunk pertinent qui a été sélectionné doit être surligné dans le document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUqNqBxnilKg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF4qhJ0Kildt"
   },
   "source": [
    "# Bonus Partie 3:\n",
    "\n",
    "Ajoutez des métadonnées (date de création de la page web, entités nommés, langues, type de fichiers...) et modifiez le code de façon à pouvoir appliquer des filtres sur la recherche"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPXs0CuWwbdmPJ04wQDPJIQ",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tal)",
   "language": "python",
   "name": "tal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
